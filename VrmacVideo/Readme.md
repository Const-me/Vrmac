# Vrmac VideoThe DLL built from this source code implements h264 media player for Linux.## MotivationCompressed video is used everywhere.Yet it’s surprisingly difficult to play unless using running on Windows and using some MS XAML framework.The only easy cross-platform way appears to be integrating a web browser, and using `<video>` element.I don’t like web browsers too much, and wanted to support media playback in my graphics library.### Why not ffmpeg?It comes with LGPL license. Being able to reuse my code in commercial projects is one of the reasons why I’m doing that at all.It’s a huge library, even just `libavcodec58` component has 113MB of dependent DLLs, that’s 70% more than the whole .NET runtime.Likely reason for that, they support many audio and video formats no one uses,either because deprecated like mpeg1, windows media or atrac, never took off like vorbis, or just too exotic.It’s not too efficient, they pass everything in the reference counted buffers in system memory.I wanted to avoid copying data as much as possible by using memory mapped I/O everywhere, for both video and audio.Last but not least, multimedia containers are extremely complex these days, I keep reading about security vulnerabilities in the parsers of these formats.I don’t trust C++ for that job, I wanted that code in a memory-safe .NET runtime.## Current StatusDespite not 100% feature complete, it already plays a lot of files. Here’s what’s supported.**Container formats:** Mpeg4, MKV**Video codec:** I only support **h264**.The library supports most profiles and levels except 10-bit, and except the ones with 4:2:2 and 4:4:4 chroma subsampling.See [this table on Wikipedia](https://en.wikipedia.org/wiki/Advanced_Video_Coding#Feature_support_in_particular_profiles) for more info.According to the output of V4L2 API, the size of h264 video the hardware can decode is limited to 1920×1920 pixels.Technically, supporting Motion JPEG is relatively simple, the Linux kernel reports the same stateful video decoder that I use for h264 can also decode MJPEG.I don’t think it gonna be useful for many people, h264 is much better, also way more popular.**Audio codecs:** All mainstream ones are supported, with the notable exceptions of MP3 and Dolby Digital Plus.Specifically, the library plays AAC, HE-AAC (untested but very likely will just work), Dolby AC-3, DTS Coherent Acoustics.About MP3, I don’t think it used much in real world anymore, I just neglected.About Dolby Digital Plus a.k.a. Enhanced AC-3, I wasn’t able to find a suitable decoder I could integrate.Almost 100% of the source code is in this repository, and is in C#.Only a few things are in C++: GLES intergation i.e. importing V4L2 buffers, audio decoders (two of them are third-party, [one is custom](https://github.com/const-me/DtsDecoder)),and couple SIMD utility functions which interleave and scale decoded audio.This is very complicated codebase. Pretty sure it has more bugs than listed below in the “Known issues” section of this readme.Still, as a proof of concept I think it’s doing great.People often scared of using .NET for real-time multimedia applications, due to GC and performance overhead.This project proves the .NET is totally fine, even on relatively slow hardware like a $50 Raspberry Pi 4.As for the remaining bugs and missing features, will do if I’ll have more time for this, or if I’ll want to reuse the code in a production software, whichever happens first.## PerformancePerformance wise, the result of skipping a multimedia framework is awesome.In my tests, CPU usage on Pi4 as reported by `top` utility is 7-8% for 720p video with stereo Dolby AC3 audio, and 13-15% for high bitrate 10 megabits/second 1080p video, with 6 audio channels in DTS format.The audio adds 1.5 megabit/second, so the total compressed bandwidth handled by the player is about 11.5 megabit/sec.While playing that high-bitrate 1080p video, `cifsd` process consumes another 1-2% of CPU time, because I play my test files from a network-mounted drive on a desktop PC.The figure is slightly higher, 14-16%, when rendering in Linux desktop as opposed to bare metal.This is a consequence of no proper vsync support in Pi4 graphics stack.The RAM use of `dotnet` process was under 80MB, very stable.By comparison, while playing the same media file VLC player consumed same 14-16% of CPU, and slightly over 100MB of RAM.My implementation even delivers slightly better video quality.When I took screenshots of 1080p video on Linux and inspected then on my Windows PC with 4k IPS display, I’ve found the same colors.That’s why I compared them at all, with these YUV formats it’s very easy to screw up the colors, due to transfer matrices, full versus limited range, chroma siting, and more shenanigans that don’t usually apply to normal RGB rendering applications.But also, to my surprise I’ve found that VLC rendering has more high-frequency noise on the video. Have no idea why it happens, based on very similar resource consumption we probably using equivalent code.The library can play videos for hours without any issues.Infamous garbage collector pauses are avoidable in modern .NET, due to value types, native stack, spans, etc.Playing FullHD video + audio consumes sizeable bandwidth in both data bytes/second and instructions/second, .NET runtime is totally fine with both.I allocate no memory while playing video, everything I need is allocated when starting the playback. Both encoded and decoded frames never hit managed memory.Video stream is read from file directly into the memory-mapped V4L2 buffers.Decoded video frames, in NV12 format, are passed to GLES for rendering using [DMA buffer sharing](https://elinux.org/images/a/a8/DMA_Buffer_Sharing-_An_Introduction.pdf) API.Audio stream is read from file into unmanaged buffers I allocate with Marshal.AllocHGlobal API (a .NET equivalent of malloc), these buffers then passed to the audio thread, which decodes them into memory mapped ALSA buffers.## Known Issues and LimitationsStreaming is not supported. The only way to play a media file, have it on the hard drive.If you have it in memory it's easy to implement, but you need the complete container in memory.Due to different pace of buffering between video and audio streams, I seek through the media file quite a lot while playing it.Raspberry Pi4 has hardware h265 = HEVC video decoder in the SoC as well, however it’s not yet available through Video 4 Linux API.Not supported by this library.HEVC is on my roadmap though, when I’ll have more time for this project, I’ll try to integrate it as well.Ideally, I would prefer HEVC decoder to be integrated into Linux first. The current Pi4 HEVC decoder API is not good, too low level.And it's not clear how to export decoded frames to EGL textures.That’s especially tricky for 10-bit decoded frames.While I know to use ARM NEON i.e. can write a few lines of C++ to convert 10 bit to 8, but I think fundamentally that’s not the correct approach.That transform needs to be coupled with YUV to RGB or some quality will be lost, and copying RGB frames would consume too much RAM bandwidth, especially in 4k.If the audio in the media file is 5.1 or 7.1, the player should downmix it to stereo while decoding to PCM. There’s no option to output digital DTS or AC3 audio.Not all combinations of codecs and containers are tested, e.g. DTS or AC3 are unlikely to play from Mpeg4 containers because I have never tested that combination, I have tested these audio formats with MKV containers.A fix should be quite simple, though, send me a pull request if you want.When the media has multiple video or audio tracks, there’s no API to select which to play, the library will use the first one. Subtitle tracks are ignored.Some rare features of codecs and containers may not work. E.g. it will refuse to load MKV files with Xiph lacing. It also requires MKV and Mpeg4 files to contain a seek index.Currently, the only thing you can do to the video frame textures - render them with the library-builtin shaders.If you want to do something more creative with the video, like applying custom shaders, you have two options.1. Easier but slower option: render the video into RGB texture, use the texture however you like.2. Harder but more efficient option: fork the library and change the renderers, `WindowsRender` and `LinuxRender` classesfrom `NetCore\Vrmac\MediaEngine\Render` folder, and/or the HLSL/GLSL shaders in that folder.I have not tested what happens when playing from slow networks, high-latency networks, or from spinning SATA hard drives.Very likely, a few adjustments are needed to make it play reliably under these conditions, or degrade gracefully i.e. pause both streams and buffer as opposed to failing with an exception.I'm not sure the channel downmixing code in DTS audio decoder DLL is correct.Specifically, I’m afraid it decreases output volume proportionally to the count of channels it downmixes.I haven’t written that code, it was part of the decoder library I’ve copy-pasted.Seek is slow on Linux, takes a few seconds to complete.I’ve implemented a brute-force algorithm: I find the keyframe, then decode and discard all video samples until reached the seek target.It’s probably possible to optimize somehow, by decoding them selectively and skipping some intermediate frames.Also it’s definitely possible to optimize seeks by small positive amount, by proceeding from a current position as opposed to starting from a key frame.Windows version (not in this repo, the media engine is in `C:\Windows\System32\MFMediaEngine.dll` and I don’t have source codes) won’t play media with DTS audio,prints an error “Windows media error code 4: The media resource is not supported”.Might need a third-party codec DLL. Other then that, it’s pretty comparable to my Linux version, plays both mpeg4 and mkv.Seek is much faster than my version on Pi 4, seek to a random location of media file only takes a small fraction of a second.## DependenciesThere’re two new runtime dependencies on Linux. Steps to install.1. [Click this link](https://packages.debian.org/buster/armhf/libfdk-aac1/download)2. Select any mirror. You should get a file named `libfdk-aac1_0.1.6-1_armhf.deb`3. Copy that file to the Pi.4. In the command line of the pi, run this command: `sudo dpkg -i ./libfdk-aac1_0.1.6-1_armhf.deb`5. You'll also need Dolby AC-3 decoder DLL:<br/>`sudo apt-get -y install liba52-0.7.4`# How it WorksOn the high level, the idea is obvious.I parse the containers, send compressed video to the hardware decoder, grab the decoded frames in video memory, import them into GLES and render.The audio stream is piped into a dedicated audio thread, which decompresses the audio and plays it with [ALSA](https://en.wikipedia.org/wiki/Advanced_Linux_Sound_Architecture).As you can see from the repository, a good implementation of that trivially simple workflow required over 20k lines of code.The main reason is the complexity of mpeg4 and h264, with these thousands of pages of specs spanning over [17+ chapters](https://en.wikipedia.org/wiki/MPEG-4#MPEG-4_Parts).MKV is not much simpler either.Luckily, MKV format developers ship [a computer-friendly XML](https://github.com/cellar-wg/matroska-specification/blob/master/ebml_matroska.xml) with the schema.I was able to auto-generate quite a lot of C# code from that XML, everything in `NetCore/VrmacVideo/Containers/MKV/Generated` folder of the repository is generated by a custom tool from that single XML file.The tool is not included but it’s very straightforward, reads the XML and emits these enums and classes.Linux [V4L2 API](https://www.kernel.org/doc/html/v4.19/media/uapi/v4l/v4l2.html) is not particularly user-friendly either.Unfortunately, the documentation there significantly deviates from the behavior I observed on my Pi4.The rest of the Linux kernel APIs I consume, [AAC decoder](https://packages.debian.org/buster/libfdk-aac1) library, and [ALSA](https://www.alsa-project.org/alsa-doc/alsa-lib/pcm.html) caused little to no issues in comparison.I had to spent some time on DTS audio decoder.The decoder shipped with Debian in `libdca0` package appears to be broken, when I call `dca_syncinfo` it indicated it will only read initial ~60% of data in the frames, and the audio didn't play.I've [made my own one](https://github.com/Const-me/DtsDecoder) based on another open source library.Despite I tried to improve a few things, DTS decoder appears to be the slowest of the audio decoders.Maybe this is due to their excessive use of 64-bit floats, these can’t be vectorized on ARM32.Maybe it’s just the bandwidth, DTS is often used for 5.1 and 7.1 audio, consumes more of these bits/second than stereo.By the way, a Windows equivalent of this whole library only took a few pages of C++ code.Windows has that functionality already implemented, in a Microsoft-supported OS [component](https://docs.microsoft.com/en-us/windows/win32/api/mfmediaengine/nn-mfmediaengine-imfmediaengine).Not the case on Linux.